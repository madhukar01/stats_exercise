\documentclass[12pt]{article}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\graphicspath{{./images}}

\author{Madhukara S Holla}
\title{Statistics Exercise 2}
\date{13th Octoner 2023}

\begin{document}
\maketitle
\newpage
\section*{Question 1}
Project members: Madhukara S Holla (Master of Science in Computer Science, 1st Year)
\\
Project description: NA

\newpage
\section*{Question 2}
\subsection*{2.a}
\subsubsection*{Anscombe's Dataset 1}
\begin{itemize}
    \item Observed pearson co-efficient: 0.8164
    \item Observed co-efficient of multiple determination: 0.6665
\end{itemize}
\includegraphics*[width=\linewidth]{graph2a1}

\newpage
\subsection*{2.a}
\subsubsection*{Anscombe's Dataset 2}
\begin{itemize}
    \item Observed pearson co-efficient: 0.8162
    \item Observed co-efficient of multiple determination: 0.6662
\end{itemize}
\includegraphics*[width=\linewidth]{graph2a2}

\newpage
\subsection*{2.a}
\subsubsection*{Anscombe's Dataset 3}
\begin{itemize}
    \item Observed pearson co-efficient: 0.8163
    \item Observed co-efficient of multiple determination: 0.6663
\end{itemize}
\includegraphics*[width=\linewidth]{graph2a3}

\newpage
\subsection*{2.a}
\subsubsection*{Anscombe's Dataset 4}
\begin{itemize}
    \item Observed pearson co-efficient: 0.8165
    \item Observed co-efficient of multiple determination: 0.6667
\end{itemize}
\includegraphics*[width=\linewidth]{graph2a4}

\newpage
\subsection*{2.b}
Lack of fit tests for Anscombe's datasets.
\\
Datasets grouped by x values (2 values per group)
\\[\baselineskip]
Null Hypothesis \(H_0\): A simple linear model is adequate to explain the systematic
variations in the data.
\\[\baselineskip]
Alternate Hypothesis \(H_a\): A linear model is not adequate and a nonlinear model
is required to capture the systematic variations in the data.

\subsubsection*{Anscombe's Dataset 1}
P value: 0.86 - Fail to reject \(H_0\).
\\
No significant lack of fit.The dataset appears to be a simple linear relationship,
and a linear regression model seems appropriate for this dataset.

\subsubsection*{Anscombe's Dataset 2}
P value: 0.03 - Reject \(H_0\) in favor of \(H_a\).
\\
Significant lack of fit. The data clearly follows a non-linear (quadratic)
relationship, indicating that the linear model does not capture all systematic
variations in the dataset.

\subsubsection*{Anscombe's Dataset 3}
P value: 0.83 - Fail to reject \(H_0\).
\\
The outlier is ignored when we group the data by x values in pairs of 2.
\\
No significant lack of fit. Since the influence of the outlier is ignored while
calculating lack of fit, the dataset appears to be a simple linear relationship.

\subsubsection*{Anscombe's Dataset 4}
P value: 0.04 - Reject \(H_0\) in favor of \(H_a\).
\\
The outlier is ignored when we group the data by x values in pairs of 2.
\\
Significant lack of fit. Test is not be appropriate due to the nature of the data.
If forced, likely a significant lack of fit.
\\[\baselineskip]
The lack of fit test assumes that there's some variation in the independent variable (x)
that corresponds to variation in the dependent variable (y).
In Dataset 4, for all but one observation, there's no variation in x.
This goes against the fundamental premise of regression that
we're trying to understand how y changes as x changes.

\newpage
\subsection*{2.c}
\begin{itemize}
    \item Patterns in residual plots can indicate non-linearity and outliers,
    helping us to identify problems with the model.
    \item Lack of fit tests provide a formal statistical test to validate
    the model assumptions.
    \item But the lack of fit tests require replicate observations in the data
    which may not be available, or it may be inappropriate to run on datasets
    like Dataset 4.
    \item Both pearson coefficient and co-efficient of multiple determination
    do not clearly indicate the goodness of fit of the model. They just indicate
    the strength of the linear relationship and proportion of variance explained
    by the model respectively.
\end{itemize}
In conclusion, we need to use multiple methods such as visualization, lack of fit
tests, and co-efficient determinations to validate the model assumptions.

\newpage
\section*{Question 3}
\subsection*{3.a}
\includegraphics*[width=\linewidth]{graph3a}
The Bonferroni adjustment controls the Familywise Error Rate (FWER).
This is the probability of making at least one Type I error when performing
multiple statistical tests.
\\
By adjusting the significance level with the Bonferroni method,
we ensure that our overall chance of incorrectly rejecting a true null
hypothesis remains at the specified level (like 5\%).

\newpage
\subsection*{3.b}
\includegraphics*[width=\linewidth]{graph3b}
The Working-Hotelling confidence method is used to construct confidence
intervals for the difference between the means of two multivariate data sets.
\\
It controls for the precision of the estimate of the difference between the means.
It provides a range of values within which we can be reasonably confident that
the true difference between the means lies.

\newpage
\subsection*{3.c}
For a simple linear regression model using least squares estimation,
\[Y = \beta_0 + \beta_1 . X + \epsilon\]
The estimates for \(\beta_0\) and \(\beta_1\) are given by:
\[b_1 = \frac{\Sigma(X_i - \bar{X})(Y_i - \bar{Y})}{\Sigma(X_i - \bar{X})^2}\]
\[b_0 = \bar{Y} - b_1.X\]
and \(\epsilon\) is the error term.
\\[\baselineskip]
\[cov(b_0, b_1) = E[(b_0 - E[b_0])(b_1 - E[b_1])]\]
\[cov(b_0, b_1) = E[b_0b_1] - E[b_0].E[b_1]\]
Substituting for \(b_0\),
\[E[b_0 b_1] = E[(\bar{Y} - b_1 \bar{X})(b_1)]\]
\[= E[\bar{Y}b_1 - b_1^2 \bar{X}]\]
\[= \bar{Y} E[b_1] - \bar{X} E[b_1^2]\]
\\
We know \(E[b_1] = \beta_1\) and
\[E[b_0] = E[\bar{Y} - b_1 \bar{X}]\]
\[= \bar{Y} - \bar{X}E[b_1] = \bar{Y} - \bar{X}.\beta_1\]
Now we have,
\[E[b_0 b_1] = \beta_1\bar{Y} - \bar{X} E[b_1^2]\]
\[E[b_0].E[b_1] = (\bar{Y} - \bar{X}.\beta_1)\beta_1\]
\\
Substituting in \(cov(b_0, b_1)\) we have,
\[cov(b_0, b_1) = \beta_1\bar{Y} - \bar{X} E[b_1^2] - (\bar{Y} - \bar{X}.\beta_1)\beta_1\]
\[= \beta_1\bar{Y} - \bar{X} E[b_1^2] - \beta_1\bar{Y} + \bar{X}.\beta_1^2\]
\[= \bar{X}.\beta_1^2 - \bar{X} E[b_1^2]\]
\\
We need a formula for \(E[b_1^2]\)
\[Var(b1) = E[b_1^2] - (E[b_1])^2 = E[b_1^2] - \beta_1^2\]
\\From linear regression,
\[Var(b_1) = \frac{\sigma^2}{\Sigma(X_i - \bar{X})^2}\]
\[E[b_1^2] - \beta_1^2 = \frac{\sigma^2}{\Sigma(X_i - \bar{X})^2}\]
\[E[b_1^2] = \beta_1^2 + \frac{\sigma^2}{\Sigma(X_i - \bar{X})^2}\]

\newpage
Substituting into \(cov(b_0, b_1)\) we have,
\[cov(b_0, b_1) = \bar{X}.\beta_1^2 - \bar{X} E[b_1^2]\]
\[=\bar{X}.\beta_1^2 - \bar{X} (\beta_1^2 + \frac{\sigma^2}{\Sigma(X_i - \bar{X})^2})\]
\[= - \frac{\sigma^2}{\Sigma(X_i - \bar{X})^2}\bar{X}\]
\[cov(b_0, b_1) = -\bar{X}.Var(b_1)\]

\newpage
\section*{Question 4}
\subsection*{4.a}
Covariance matrix for the data:
\[\begin{bmatrix}
    25.23 & 24.29 & 8.39 & 21.63 \\
    24.29 & 27.4  & 1.62 & 23.47 \\
    8.39  & 1.62  & 13.3 & 2.65  \\
    21.63 & 23.47 & 2.65 & 26.07 \\
    \end{bmatrix}\]
Row 1: Covariance of Triceps with Triceps, Thigh, Midarm, Bodyfat
\\
Row 2: Covariance of Thigh with Triceps, Thigh, Midarm, Bodyfat
\\
Row 3: Covariance of Midarm with Triceps, Thigh, Midarm, Bodyfat
\\
Row 4: Covariance of Bodyfat with Triceps, Thigh, Midarm, Bodyfat

\newpage
\subsection*{4.b}
\subsubsection*{Triceps as a linear predictor of Bodyfat}
\includegraphics*[width=\linewidth]{graph4b1}
Summary of the data:
\begin{itemize}
    \item Slope: 0.8572
    \item Intercept: -1.4961
    \item Mean squared error: 7.9511
    \item \(R^2\): 0.7111
    \item The positive value of the slope signifies a positive linear association
    between Triceps and Body fat.
    \item An MSE of 7.9511 indicates there is a significant amount of error in
    the estimation of Bodyfat from Triceps.
    \item \(R^2\) value of 0.71 indicates that \(\approx\) 70\% of the variability
    in Bodyfat can be explained by Triceps.
\end{itemize}
From the graph and summary, Triceps show a strong linear association with Bodyfat.
\newpage
\subsubsection*{Thigh as a linear predictor of Bodyfat}
\includegraphics*[width=\linewidth]{graph4b2}
Summary of the data:
\begin{itemize}
    \item Slope: 0.8565
    \item Intercept: -23.6345
    \item Mean squared error: 6.3013
    \item \(R^2\): 0.7710
    \item The positive value of the slope signifies a positive linear
    association between Thigh and Body fat.
    \item An MSE of 6.3013 indicates there is still error in the estimation
    of body fat, but the error is less than that of Triceps.
    \item \(R^2\) value of 0.77 indicates that \(\approx\) 77\% of the variability
    in Bodyfat can be explained by Thigh. This association is even stronger than
    that of Triceps.
\end{itemize}
From the graph and summary, Thigh show a strong linear association with Bodyfat.F
From a lower MSE and higher \(R^2\) value, we can conclude that Thigh is a better
predictor of Bodyfat than Triceps.

\newpage
\subsubsection*{Midarm as a linear predictor of Bodyfat}
\includegraphics*[width=\linewidth]{graph4b3}
Summary of the data:
\begin{itemize}
    \item Slope: 0.8565
    \item Intercept: -23.6345
    \item Mean squared error: 6.3013
    \item \(R^2\) value: 0.0203
    \item The positive value of the slope signifies a positive linear relationship
    with the Bodyfat. But from the graph we can see that the model has several
    outliers and does not fit the data well.
    \item MSE term indicates a similar variance to that of Triceps and Thigh, but
    given the low \(R^2\) value, this can be misleading.
    \item \(R^2\) value of 0.02 is considerably low and indicates that Midarm is
    not a good predictor of Bodyfat.
\end{itemize}
Despite the slope and MSE values for midarm being similar to that of Triceps and
Thigh, the graph and \(R^2\) value indicate that Midarm does not have a strong
linear association with Bodyfat.

\newpage
\subsection*{4.c}
Why we may want to fit a multi-variate model:
\begin{itemize}
    \item To capture non-linearity: By including term \({Tricep}^2\), we are
    exploring the possibility of a non-linear relationship between Tricep and
    Bodyfat.
    \item Model flexibility: Adding polynomial terms offers more flexibility
    in capturing patterns in the data. This can lead to better predictions if
    underlying relationship is non-linear.
\end{itemize}
Summary of multivariate linear regression model with \(Tricep\) and \({Tricep}^2\):
\begin{itemize}
    \item Intercept: -6.1523
    \item Coefficient for \(Tricep\): 1.2612
    \item Coefficient for \({Tricep}^2\): -0.0083
    \item Mean squared error: 8.3777
    \item \(R^2\) value: 0.7125
\end{itemize}
From the summary, we can see that the co-efficient for \({Tricep}^2\) is
\(\approx\) 0. This means that the relationship between Tricep and Bodyfat is
linear.
\\
Due to this, the value of \(R^2\) is similar to that of the simple linear model
with Tricep as a predictor. This indicates that the multivariate model does not
offer any significant improvement over the simple linear model.

\newpage
\subsection*{4.d}
\end{document}
